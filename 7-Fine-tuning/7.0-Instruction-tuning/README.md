> 原文地址 [www.ibm.com](https://www.ibm.com/topics/instruction-tuning?mhsrc=ibmsearch_a&mhq=Low-Rank%20Adaptation%20of%20LLMs)

**Published:** 5 April 2024  
**Contributors:** Dave Bergmann  

什么是指令调优？
--------

_指令调优_是一种在带有标签的指令提示和相应输出数据集上[微调](https://www.ibm.com/topics/fine-tuning)大型语言模型 [（LLM）](https://www.ibm.com/topics/large-language-models) 的技术。它不仅可以提高特定任务的模型性能，还可以提高一般遵循指令的性能，从而有助于调整预训练模型以适应实际使用。

指令调优是更广泛的微调技术类别的一个子集，用于使预训练的基础模型适应下游任务。[基础模型](https://research.ibm.com/blog/what-are-foundation-models)可以针对各种目的进行微调，从样式定制到补充预训练模型的核心知识和词汇，再到优化特定用例的性能。尽管微调并非任何特定领域或[人工智能模型](https://www.ibm.com/topics/ai-model)架构所独有，但它已成为 LLM 生命周期中不可或缺的一部分。例如，Meta 的 [Llama 2 模型系列](https://www.ibm.com/topics/llama-2)（有多种尺寸）作为基础模型、针对对话进行微调的变体 （_Llama-2-chat_） 和针对编码进行微调的变体 （_Code Llama_）。

指令调优与其他微调技术并不互斥。例如，聊天模型通常同时进行指令调整和[_来自人类反馈的强化学习_ （RLHF），](https://www.ibm.com/topics/rlhf)这是一种微调技术，旨在提高帮助和诚实等抽象品质; 为编码而微调的模型通常会进行指令调优（以广泛优化指令后续响应）和对编程特定数据的额外微调（以增强模型的编码语法和词汇知识）。

虽然 LLM 的起源可以追溯到 2017 年的 “Attention is All You Need” 论文，该论文将大规模[转换器模型](https://www.ibm.com/topics/transformer-model)引入[自然语言处理 （NLP）](https://www.ibm.com/topics/natural-language-processing) 任务，但指令调优和 RLHF 的结合_——_分别由 Google（2021 年）1 和 OpenAI（2022 年）2 的有影响力的论文推动——产生了现代 LLM，随着 ChatGPT 的推出，开启了当前的[生成式 AI](https://research.ibm.com/blog/what-is-generative-AI) 时代。

为什么要进行指令调整 LLM？
---------------

与大多数微调技术一样，指令调优的实用性在于，预训练的 LLM 没有针对对话或指令跟踪进行优化。从字面意义上讲，LLM 不_响应_提示：它们只_将文本附加到提示中。_指令调整有助于使附加的文本更有用。

_自回归_语言模型的预训练过程（用于生成文本的 LLM，如 Meta 的 [Llama 2](https://www.ibm.com/topics/llama-2)、OpenAI 的 GPT、Google 的 Gemini 或 [IBM 的 Granite](https://www.ibm.com/downloads/cas/X9W4O6BM)）优化了这些 LLM，以简单地预测给定序列中的下一个单词，直到它完成。

LLM 使用[_自我监督学习_](https://www.ibm.com/topics/self-supervised-learning)对大量书面内容进行预训练。在预训练中，自回归模型在文本样本的开头提供，并重复执行预测序列中下一个单词的任务，直到摘录结束。对于每个预测，原始样本句子的实际下一个单词用作 “基本事实”。通过迭代调整模型参数[（应用于神经](https://www.ibm.com/topics/gradient-descent)网络中每个节点发生的数学运算的不同权重和偏差）的优化算法，使模型的预测更接近原始文本，模型可以 “学习” 其训练数据中的语言模式（以及，通过扩展，这些语言模式中传达的“知识”）。

尽管这种预训练过程赋予了生成语言连贯文本的令人印象深刻的能力，但它并不一定使模型性能与人类用户的实际需求保持一致。如果不进行微调，基础模型可能会对 “_教我如何烤面包_” 的提示和 “_在家用烤箱中_” 做出响应。这是一种语法合理的句子完成方式，但不是用户想要的。

然而，为任何特定目的（如遵循指示）预先训练 LLM 是不切实际的。“大型语言模型”中的 “大” 是指这些模型通常具有数十亿个参数：从头开始训练这些大型模型需要大量的精力、时间、计算资源和训练数据。相反，对已经训练好的 LLM 进行微调需要的数据要少得多，尤其是在使用_参数高效微调_ （PEFT） 方法（如部分微调或_低秩自适应_ （LoRA））时_，_只需要计算需求的一小部分。

虽然微调可以通过几乎任何机器学习范式来实现，包括强化学习、[半监督学习](https://www.ibm.com/topics/semi-supervised-learning)或其他自监督学习，但指令调优需要对标记（_输入、输出_）对进行[监督学习](https://www.ibm.com/topics/supervised-learning)。指令调优与其他形式的监督微调 （SFT） 的区别在于，指令数据集中的_输入_样本完全由类似于用户在提示中可能提出的请求的任务组成; _这些结果_表明，对这些请求的回应是可取的。在调整模型权重以使 LLM 的输出类似于指令数据集中的示例时，LLM 通过附加包含烘焙面包实际建议的文本来 “学习” 以响应“教_我如何烤面包_” 之类的提示。

因此，指令调优有助于弥合模型的基本目标（下一个单词预测）与用户让模型遵循指令并执行特定任务的目标之间的差距。这使得模型行为更加有用和可预测。

How does instruction tuning work?
---------------------------------

在各种指令跟踪任务的标记数据集上微调 LLM 可以提高总体上遵循指令的能力，从而减少有效提示所需的上下文信息量。指令数据集可以是人为的，也可以由另一个 LLM 生成。

正如 Google Research 2022 年颇具影响力的论文 “微调语言模型是[零样本学习者](https://www.ibm.com/topics/zero-shot-learning)” 中所阐述的那样，指令调优的目标是提高 LLM 对 NLP 指令的响应能力。为了做到这一点，指令调优 “结合了预训练微调和提示范式的吸引人的方面”。从本质上讲，通过将提示工程原理有机地融入监督微调中，指令调优减少了从微调模型中引出有用、准确响应所需的[提示工程](https://www.ibm.com/topics/prompt-engineering)和[少量](https://www.ibm.com/topics/few-shot-learning)样本样本的数量。1

指令数据集中的每个训练样本都包含三个元素：

*   **一个指令：** 指定给定任务的自然语言文本_输入_。例如，“_将这句话从英语翻译成西班牙语”。_
*   **附加信息：** 可选的补充信息，提供与手头任务相关的上下文。例如，阅读理解任务的输入可能包含一段简短的段落（然后指示模型回答有关它的给定问题）。
*   **期望输出：** 根据提供的说明和上下文，给定提示的目标_输出_（响应）。这将作为一个基本事实，根据该事实来评估和优化模型的预测。

谷歌的论文指出，他们的 LaMDA-PT 模型的指令调整变体，被称为 LAN（_F_inetuned _La_nguage _N_et），在自然表达为指令的任务上经历了最大的改进，如翻译、问答、阅读理解和自然语言推理（NLI）——确定给定的 “假设” 是否在逻辑上遵循给定的 “前提” 的任务。

为了解释这一点，FLAN 论文指出了 Brown 等人在 2020 年为原始 GPT-3 模型发布的研究论文中所做的一个观察结果：为什么预训练的 LLM（没有额外的微调）在处理 NLI 等任务时遇到困难的一个解释是，类似于典型 NLI 任务的段落不太可能在用于自我监督预训练的未标记数据语料库中自然发生。3 相反，对于更类似于预训练的简单语言建模目标的任务，例如最终需要模型正确完成句子的常识推理任务，指令在很大程度上是多余的（因此指令调整带来的好处较少）。

也许最重要的是，这篇论文证明，向指令调优数据集添加额外的任务可以提高指令调优模型的性能，即使在指令数据集中没有表示的新任务上也是如此。这就是指令调优的根本好处：全面提高了模型遵循一般指令的能力。

Instruction tuning vs. multi-task fine-tuning
---------------------------------------------

FLAN 的论文还包括一项消融研究，该研究探讨了指令微调的明显好处是由于指令本身，还是仅仅归因于在多个 NLP 任务上微调模型。为了检查指令在微调中的作用，消融研究在三种不同的设置上微调了基础模型：

*   **无模板：** 只有输入是向模型提供输出。例如，翻译任务的输入将是 “_狗跑”_，目标输出将是 “_le chien court”。_
*   **数据集名称：** 每个输入前面都有任务和数据集的名称。在我们的翻译示例中，从 WMT 20144 数据集集合中提取的输入将是 “_[翻译：WMT 14 到法语] 狗跑了”。_
*   **FLAN 说明：** 输入遵循指令调优原则。对于此翻译示例，输入将为 “_请将此句子翻译成法语：'狗跑了。_

然后，消融研究测量了每个微调语言模型在一系列零样本指令跟踪任务上的结果。指令调优模型的准确率比 “无模板” 模型高 18% 以上，比 “数据集名称” 模型准确率高 8% 以上。这表明，使用指令本身进行训练对于提高看不见的任务的零样本性能至关重要。

Chain-of-thought (CoT) fine-tuning
----------------------------------

思维链 （CoT） 提示要求 LLM 不仅要回答一个问题，还要生成它如何得出答案的理由。这可以通过使用顺序推理示例的少样本提示来实现，或者简单地在提示的末尾附加 “_逐步思考_”。研究表明，CoT 提示可以显着增强大型模型在各种算术、符号推理和其他逻辑推理任务中的零样本能力。5 Wei 等人发现，在指令数据集中不包括 CoT 任务的指令调优会显著_降低_ CoT 评估的模型性能，但添加 CoT 数据集可以提高所有评估的性能。6

此外，他们的研究发现，对 CoT 任务进行指令微调——无论有没有少数样本样本——都可以提高模型在零样本设置下的 CoT 推理能力。对这种好处的直观理解是，通过微调以逻辑步骤解决问题，而不是跳到一个看起来只是在语言上连贯的答案，模型学会了更好地产生和应用自己的推理技能。

Instruction-tuning datasets
---------------------------

存在许多用于指令调优 LLM 的数据集，其中许多是开源的。这些数据集可以包含直接编写（或收集）的自然语言（_指令、输出_）对，使用模板将现有的注释数据集转换为指令，甚至使用其他 LLM 来生成示例。  
 

#### **人工创建的数据集**

虽然直接编写（_指令、输出_）对很简单，但这是一个劳动密集型过程，最终需要花费大量的时间和成本。已经提出了各种方法将自然语言数据集转换为指令，通常是通过应用模板。多个开源人工制作的数据集的发布有助于支付对有机数据进行微调的成本。

著名的开源人工创建的指令数据集包括：

*   **馅饼：** 首先用于微调 Google 的 LaMDA-PT 模型，产生原始的 FLAN 模型，此后 Flan 数据集经过改进并用于微调许多 LLM。 在 Flan 上微调的主要模型包括 FLAN-T5、Flan-UL2 和 Flan-PaLM 540B（也称为 FLAN-T5-XXL）。
*   **OpenAssistant：**OpenAssistant Conversations 是一个人工制作的多语言对话语料库，专注于助手式的对话交流。它由 91,829 个用户提示和 69,614 个助手回复组成，这些回复来自 35 种不同语言的 66,497 个对话树。
*   **洋娃娃：** Dolly 是一个包含 15,000 个人工生成对话实例的英语数据集，旨在使 LLM 能够以类似于 ChatGPT 的对话驱动模式与用户交互。它涵盖了广泛的任务和人类行为，包括总结、信息提取、头脑风暴、创意写作、分类和问答。  
     

#### **LLM 生成的数据集**

由于手动生成指令和目标输出所需的成本和劳动力过高，许多指令数据集使用较大的 LLM 的响应来生成提示、输出或两者兼而有之。使用 LLM 生成的数据集通常具有教授较小模型以模拟较大模型行为的额外效果，有时是出于教师 / 学习者的动态考虑。

*   **自我指导：** Self-Instruct 是使用 InstructGPT 构建的，InstructGPT 本身就是 GPT-3 的指令调优版本。作者提供了自然语言的 “种子任务”，并促使 InstructGPT 生成更多示例，最终产生了 52,000 条训练指令。斯坦福大学的研究人员使用改进的自我指导方法来生成 Alpaca 的训练数据，_Alpaca_ 是 LLaMA 的第一个指令调整变体。值得注意的是，Alpaca 在 Self-Instruct 数据集上的表现略优于 InstructGPT 的基准。7
*   **Evol-Instruct：** 顾名思义，Evol-Instruct 提出了对 Self-Instruct 方法的演变，使用_深入_和_广度_的策略重写指令。前者通过增加约束、增加推理步骤和使输入复杂化等措施来发展指令以增加指令的复杂性。后者 “改变” 了先前的指令，以增加数据集的多样性和主题覆盖率。Evol-Instruct 是在 _WizardLM_ 的研究论文中介绍的，该论文详细介绍了如何使用 Evol-Instruct 来微调 LLaMA。8
*   **ShareGPT：**ShareGPT.com 包含用户生成的他们与 ChatGPT 的交流存储库。Vicuna 是 LLaMA 的显着微调，它使用了 ShareGPT 的 70,000 条对话记录，并为多轮对话定制了他们的选择。9
*   **OpenOrca 的：** OpenOrca 是增强的 [Flan Collection](https://arxiv.org/abs/2301.13688)（链接位于 ibm.com 之外）数据的集合。它旨在复制 Microsoft 用于训练 _Orca_ 的数据集，该数据集探索了明确侧重于优化较大模型的使用以通过模仿学习来优化较小 LLM 的方法。10

随着 LLM 功能的增加，LLM 生成的指令调优数据集的效用也同样增加。2023 年的一篇论文复制了 Alpaca 微调范式——该范式根据 InstructGPT 生成的指令微调 LLaMA——同时使用 _GPT-4_ 并行重复该过程以生成指令。他们称之为 LLaMA-GPT4 的合成模型显着优于 Alpaca 等价物的 “乐于助人” 分数，并且在 “帮助性”、“诚实” 和“无害”的衡量标准上接近 GPT-4 本身。11

Challenges and limitations of instruction tuning
------------------------------------------------

尽管指令调优技术在 LLM 中取得了重要进展，但仍有工作要做，以使指令调优数据集多样化并充分阐明其优势。

指令调优的主要挑战是创建用于微调的高质量指令。制作适当大的指令数据集所需的资源将指令集中到少数开源数据集，这可能会降低模型多样性。尽管使用更大的专有 LLM 来生成指令有助于降低成本，但这有一个潜在的缺点，即强化了这些专有 LLM 在开源 LLM 范围内的偏见和缺点。为了规避人类研究人员的固有偏见，专有模型通常用于_评估_较小模型的性能，这一事实使这个问题变得更加复杂。

在技术层面上，一些研究人员提出了一些担忧，即使用较大的模型来改进较小的模型可能有助于最小的模型模仿较大的模型的_风格，_但不能模仿它们的实际_功能_。2023 年的一项实证研究表明，通过教学调整获得的许多令人印象深刻的表现增益可能来自拾取肤浅的模式，而不是逻辑推理的更真实的改进。12

同样，其他研究人员认为，一些报告的改进可能在某种程度上取决于评估指令调整模型性能对与指令训练数据集密切相关的任务的依赖性。通过对以这种方式调整的模型指令进行更有针对性的测试，Gudibande 等人得出结论，“改进开源模型的最高杠杆作用是解决开发更好的基础（语言模型）的艰巨挑战，而不是走模仿专有系统的捷径。13

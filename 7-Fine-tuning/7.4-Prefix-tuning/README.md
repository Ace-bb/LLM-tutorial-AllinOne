# Prefix-tuning

核心思想是在模型的输入词的嵌入向量前增加了一个Prefix Token，即前缀向量，这种前缀不是自然语言，而是向量形式的Token，被称为虚拟Token。为了保证这些虚拟Token的有效性，Prefix-tuning在Transformer的所有层中都增加了这种前缀向量。新增的一系列向量连续的、可训练的任务特定前缀（prefix），通过这些前缀来引导模型的注意力和生成过程，而不需要对模型的主体参数进行更新。
技术特点
1）参数效率：Prefix Tuning 只需要对前缀参数进行训练，而模型的主体参数保持不变，大大减少了需要训练的参数数量。
2）动态更新：在训练过程中，Prefix Tuning 动态地选择需要更新的参数，从而逐步引导模型适应特定任务。
3）任务特定：前缀是针对特定任务设计的，可以有效地引导模型的注意力，使其更专注于任务相关的信息。
4）连续可微：与基于离散模板的方法相比，Prefix Tuning 使用的是连续的向量序列，这使得优化过程更加稳定和高效。

论文链接：https://arxiv.org/pdf/2101.00190.pdf

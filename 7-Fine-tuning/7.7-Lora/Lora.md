> åŸæ–‡åœ°å€ [martinlwx.github.io](https://martinlwx.github.io/zh-cn/lora-finetuning/)

> ä»‹ç»äº† LoRA å¾®è°ƒçš„åŸç†ï¼Œä»¥åŠç›¸å…³çš„æºä»£ç åˆ†æ

![](../_img/lora.jpg)

è‡ªä» LLM æ—¶ä»£åˆ°æ¥ä¹‹åï¼Œå¦‚ä½•å¾®è°ƒ LLM æˆä¸ºäº†ä¸€ä¸ªéš¾é¢˜ï¼Œå› ä¸º LLM çš„æ¨¡å‹å®åœ¨æ˜¯å¤ªå¤§äº†ï¼Œå¾ˆéš¾åšå…¨é‡å¾®è°ƒæ›´æ–°æ‰€æœ‰å‚æ•°ã€‚å¯é€‰çš„è·¯çº¿æœ‰ï¼šå†»ç»“æ•´ä¸ªæ¨¡å‹åš Prompt tuning æˆ–è€… In-context Learningï¼›å†»ç»“æ•´ä¸ªæ¨¡å‹_ä½†æ˜¯_ä¼šæ’å…¥å¯è®­ç»ƒçš„æ¨¡å—ã€‚ä»Šå¤©è¦ä»‹ç»çš„ LoRA(**Lo**w-**R**ank **A**daptation) å°±å¯¹åº”äº†åè€…çš„æŠ€æœ¯è·¯çº¿ï¼Œè¿™æ˜¯å¾®è½¯å›¢é˜Ÿçš„å·¥ä½œ 

LoRA çš„æ€æƒ³å…¶å®æŒºç®€å•ï¼Œæˆ‘ä»¬çŸ¥é“ï¼Œåœ¨æ·±åº¦å­¦ä¹ é‡Œé¢ï¼Œæ¨¡å‹çš„å‚æ•°æ˜¯é€šè¿‡æ¢¯åº¦ä¸‹é™è¿›è¡Œæ›´æ–°çš„ï¼Œè€ƒè™‘ä¸€ä¸ªçŸ©é˜µ $W_0 â€‹âˆˆ R^{dÃ—d}$ï¼ˆè¿™é‡Œçš„ä¸‹æ ‡ 0 è¡¨ç¤ºå®ƒæ˜¯åˆå§‹å€¼ï¼‰ï¼Œå¯ä»¥ç”¨ $Î”W$ è¡¨ç¤ºå®ƒæœ€åè®­ç»ƒå®Œæˆçš„æ—¶å€™ç›¸å¯¹äºä¸€å¼€å§‹çš„åˆå§‹å€¼çš„_**å˜åŒ–é‡**_ï¼Œé‚£ä¹ˆè®­ç»ƒå®Œæˆä¹‹åè¿™ä¸ªçŸ©é˜µçš„å‚æ•°ä¼šæ˜¯

$$W0â€‹+Î”W$$

LoRA å¾®è°ƒè¦è§£å†³çš„é—®é¢˜æ˜¯ 
- èƒ½ä¸èƒ½åœ¨_**å†»ç»“**_ $W_0$â€‹ çš„æƒ…å†µä¸‹ï¼Œæ±‚è§£å‡º $Î”W$ï¼Ÿè€Œä¸”æ±‚è§£çš„å¼€é”€è¦å°½å¯èƒ½ä½ï¼Ÿè¿™æ˜¯å¯ä»¥çš„ï¼Œå› ä¸ºç ”ç©¶äººå‘˜å‘ç°è®­ç»ƒå®Œæˆåçš„ LLM çš„æ¨¡å‹æƒé‡çš„ Intrinsic rank æ¯”è¾ƒä½ï¼Œäºæ˜¯ä½œè€…å‡è®¾ $Î”W$ ä¹Ÿæ˜¯å¦‚æ­¤ï¼Œé‚£ä¹ˆæˆ‘ä»¬å°±å¯ä»¥å¯¹ $Î”W$ åšä½ç§©åˆ†è§£ï¼Œæœ€åå®éªŒè¡¨æ˜è¿™ä¸ªå‡è®¾æ˜¯æ­£ç¡®çš„ï¼ŒLoRA å¾®è°ƒæ•ˆæœæŒºå¥½ï¼Œä½ç§©åˆ†è§£æŒ‡çš„æ˜¯

$Î”W=BA$

å…¶ä¸­ $B âˆˆ R^{dÃ—r}$ï¼Œ$A âˆˆ R^{rÃ—d}$ï¼Œ$B$ é‡‡ç”¨é›¶å€¼åˆå§‹åŒ–ï¼Œ$A$ åˆ™æ˜¯é‡‡ç”¨é«˜æ–¯å‡½æ•°åˆå§‹åŒ–ï¼Œè¿™æ ·åœ¨ä¸€å¼€å§‹è®­ç»ƒçš„æ—¶å€™ $BA=0$ï¼ŒLoRA æ¨¡å—å¯¹æœ¬æ¥çš„æ¨¡å‹ä¸é€ æˆå½±å“

å¦‚æœè¾“å…¥æ˜¯ $x$ï¼Œé‚£ä¹ˆ LoRA çš„è®¡ç®—æ–¹å¼å°±æ˜¯

$$ W_0â€‹x+ \frac{r}{Î±}â€‹ Î”Wx = W0â€‹x + \frac{r}{Î±}â€‹BAx $$

è¿™é‡Œçš„ Î± å°±æ˜¯æ”¾ç¼©å› å­ï¼Œr åˆ™æ˜¯é™ç»´çŸ©é˜µé™ç»´åçš„å¤§å°ï¼Œæ•´ä½“ä½œä¸ºä¸€ä¸ªç¼©æ”¾å› å­ï¼Œåœ¨åé¢çš„æºç åˆ†æä¼šæœ‰æ‰€ä½“ç°

LoRA å¾®è°ƒ_è®­ç»ƒ_çš„æ—¶å€™åªéœ€è¦é€šè¿‡æ¢¯åº¦ä¸‹é™æ›´æ–° B å’Œ Aï¼Œè€Œ_æ¨ç†_çš„æ—¶å€™ï¼Œå¯ä»¥ç›´æ¥æŠŠ W0â€‹ å’Œ BA åˆå¹¶èµ·æ¥ï¼Œ_å°±åƒ_ LoRA æ¨¡å—ä¸å­˜åœ¨ä¸€æ ·ã€‚è¿™æ˜¯ LoRA ä¸€ä¸ªæ˜¾è‘—ä¼˜åŠ¿ï¼šå®ƒå¹¶ä¸ä¼šå¸¦æ¥æ¨ç†å»¶è¿ŸğŸ‘

å¦å¤–å¯ä»¥è®¡ç®—ä¸€ä¸‹ä½¿ç”¨ LoRA ä¹‹åï¼Œå¯å­¦ä¹ çš„å‚æ•°é‡çš„å˜åŒ–

 $$ (W_0 â€‹+ \frac{r}{Î±}â€‹Î”W)x = W_0â€‹x+\frac{r}{Î±}â€‹Î”Wx = W_0â€‹x+\frac{r}{Î±}â€‹BAx $$

è¿™é‡Œçš„ $rÂ«d$ï¼Œå› æ­¤å¯ä»¥å‡å°‘å¾ˆå¤šéœ€è¦è®­ç»ƒçš„æ¨¡å‹å‚æ•°ï¼Œæ‰€ä»¥ LoRA æ˜¯_å‚æ•°é«˜æ•ˆ_çš„å¾®è°ƒæ–¹æ³•ğŸ‘

**è¿˜å‰©ä¸‹ä¸¤ä¸ªé—®é¢˜ â€”â€” LoRA è¦åŠ åœ¨ Transformer çš„å“ªä¸€ä¸ªéƒ¨åˆ†ï¼Ÿæœ€ä½³çš„ `r` æ˜¯å¤šå°‘**ï¼Ÿ

*   åœ¨è®ºæ–‡çš„ 7.1 é‡Œé¢ï¼Œä½œè€…å¯¹æ¯”ä¹‹åå‘ç°**åŒæ—¶åŠ åœ¨ Wqâ€‹ å’Œ Wvâ€‹ ä¸Šçš„æ•ˆæœæ˜¯æœ€å¥½çš„** 
*   åœ¨è®ºæ–‡çš„ 7.2 é‡Œé¢ï¼Œä½œè€…å‘ç°**ä¸€å‘³å¢å¤§ `r` å¹¶æ²¡æœ‰å¸¦æ¥å¤ªå¤šçš„æå‡ï¼Œ`4 ~ 8` æ•ˆæœå°±ä¸é”™äº†**ï¼Œè¿™éªŒè¯äº† LLM çš„æ¨¡å‹æƒé‡çš„ Intrinsic rank ç¡®å®æ¯”è¾ƒä½ 

Huggingface çš„ [peft](https://github.com/huggingface/peft) å°±æ”¯æŒ LoRA å¾®è°ƒï¼Œåœ¨ Github ä»“åº“çš„ `README.md` æ–‡ä»¶å°±ç»™äº†ä¸€ä¸ªä¾‹å­ï¼Œåªéœ€è¦ç”¨ `LoraConfig` å¯¹å‚æ•°è¿›è¡Œé…ç½®ï¼Œç„¶åç”¨ `get_peft_model` å°±å®Œæˆäº†å¯¹æ¨¡å‹çš„æ”¹é€ ï¼Œå°±å¯ä»¥ç”¨äºåç»­è®­ç»ƒäº†

```python
from transformers import AutoModelForCausalLM
from peft import get_peft_config, get_peft_model, LoraConfig, TaskType

model_name_or_path = "facebook/opt-350m"

peft_config = LoraConfig(
    task_type=TaskType.CAUSAL_LM,
    r=8,
    lora_alpha=32,
    lora_dropout=0.1,
)

model = AutoModelForCausalLM.from_pretrained(model_name_or_path)
model = get_peft_model(model, peft_config)
model.print_trainable_parameters()
# output: trainable params: 786,432
#      || all params: 331,982,848
#      || trainable%: 0.2368893467652883

```

è®­ç»ƒå®Œæˆä¹‹åéœ€è¦_ä¿å­˜æ¨¡å‹_ï¼Œè°ƒç”¨ `model.save_pretrained(output_dir)` å³å¯ï¼Œå…¶ä¸­ `output_dir` å°±æ˜¯è¦ä¿å­˜çš„è·¯å¾„ï¼Œè§‚å¯Ÿç›®å½•ç»“æ„å¯ä»¥å‘ç°ï¼Œåªä¿å­˜ LoRA æ¨¡å—çš„æƒé‡è€Œä¸éœ€è¦ä¿å­˜æ•´ä¸ªæ¨¡å‹ã€‚ç›®å½•ç»“æ„é•¿ä¸‹é¢è¿™æ ·

```
output_dir
â”œâ”€â”€ README.md
â”œâ”€â”€ adapter_config.json
â””â”€â”€ adapter_model.bin

```

åç»­è¦_åŠ è½½æ¨¡å‹_ä¹Ÿå¾ˆç®€å•

```python
from peft import AutoPeftModelForCausalLM

peft_model_name_or_path = "./output_dir"

model = AutoPeftModelForCausalLM.from_pretrained(peft_model_name_or_path)

```

> ä¸‹é¢çš„ä»£ç æˆ‘å»æ‰äº†ä¸€äº›æ— å…³çš„ä»£ç ï¼Œ_æ¯”å¦‚é”™è¯¯å¤„ç†ã€è¿‡äºå†—é•¿çš„ç±»å‹æç¤ºç­‰_ã€‚å‚è€ƒçš„æºä»£ç æ˜¯ `peft 0.5.0` ç‰ˆæœ¬

LoRA å¾®è°ƒçš„æ ¸å¿ƒæ˜¯ `LoraModel` ç±»

```python
class LoraModel(BaseTuner):
    def __init__(self, model, config, adapter_name) -> None:
        super().__init__(model, config, adapter_name)
	...

```

`LoraModel` ç»§æ‰¿è‡ª `BaseTuner` å¹¶ä¸”è°ƒç”¨äº† `BaseTuner` çš„æ„é€ å‡½æ•°ï¼Œæ‰€ä»¥æˆ‘ä»¬åº”è¯¥å»æŸ¥çœ‹çˆ¶ç±»æ„é€ å‡½æ•°åšäº†ä»€ä¹ˆï¼Œå®šä½åˆ°çˆ¶ç±» `BaseTuner`

```python
class BaseTuner(nn.Module, ABC):
    def __init__(self, model, peft_config, adapter_name) -> None:
        super().__init__()
        self.model = model
        self.inject_adapter(self.model, adapter_name)
        self.model.peft_config = self.peft_config

```

æ ¸å¿ƒåº”è¯¥æ˜¯è¿™ä¸ª `inject_adapter` æ–¹æ³•

```python
class BaseTuner(nn.Module, ABC):
    def inject_adapter(self, model: nn.Module, adapter_name: str):
        peft_config = self.peft_config[adapter_name]

        is_target_modules_in_base_model = False
        key_list = [key for key, _ in model.named_modules()]

        peft_config = self._prepare_adapter_config(peft_config, model_config)

        for key in key_list:
            if not self._check_target_module_exists(peft_config, key):
                continue

            is_target_modules_in_base_model = True
            parent, target, target_name = _get_submodules(model, key)

            optionnal_kwargs = {
                "loaded_in_8bit": getattr(model, "is_loaded_in_8bit", False),
                "loaded_in_4bit": getattr(model, "is_loaded_in_4bit", False),
                "current_key": key,
            }

            self._create_and_replace(
                peft_config,
                adapter_name,
                target,
                target_name,
                parent,
                **optionnal_kwargs,
            )

        # æ˜¾è€Œæ˜“è§ï¼Œå°±æ˜¯æ ‡è®°ä¸€ä¸‹åªæœ‰ adapter æ˜¯å¯ä»¥è®­ç»ƒçš„
        self._mark_only_adapters_as_trainable()

        # å¦‚æœæ˜¯æ¨ç†é˜¶æ®µï¼Œé‚£ä¹ˆå°±æ‰€æœ‰çš„å‚æ•°éƒ½å†»ç»“
        if self.peft_config[adapter_name].inference_mode:
            for n, p in self.model.named_parameters():
                if adapter_name in n:
                    p.requires_grad = False

```

å¯ä»¥çœ‹åˆ° `inject_adapters` åšçš„äº‹æƒ…æ— éå°±æ˜¯éå†æ¯ä¸ª Module çœ‹å“ªäº›æ˜¯è¦ä¿®æ”¹çš„ï¼Œé‡ç‚¹æ˜¯è¿™ä¸ª `_create_and_replace` æ–¹æ³•ï¼Œäºæ˜¯æ¥ä¸‹æ¥å°±å®šä½åˆ°äº† `LoraModel` çš„ `_create_and_replace` æ–¹æ³•

```python
class LoraModel(BaseTuner):
    def _create_and_replace(
        self,
        lora_config,
        adapter_name,
        target,
        target_name,
        parent,
        **optionnal_kwargs,
    ):
        bias = hasattr(target, "bias") and target.bias is not None
        kwargs = {
            "r": lora_config.r,
            "lora_alpha": lora_config.lora_alpha,
            "lora_dropout": lora_config.lora_dropout,
            "fan_in_fan_out": lora_config.fan_in_fan_out,
            "init_lora_weights": lora_config.init_lora_weights,
        }
        kwargs["loaded_in_8bit"] = optionnal_kwargs.pop("loaded_in_8bit", False)
        kwargs["loaded_in_4bit"] = optionnal_kwargs.pop("loaded_in_4bit", False)
        kwargs["bias"] = bias

        if isinstance(target, LoraLayer) and isinstance(target, torch.nn.Conv2d):
            ...
        else:
            new_module = self._create_new_module(
                lora_config, adapter_name, target, **kwargs
            )
            self._replace_module(parent, target_name, new_module, target)

```

è¿™é‡Œçœ‹åˆ°äº† `LoraLayer`ï¼ŒçŒœæµ‹åº”è¯¥æ˜¯æ”¹å˜ä¹‹åçš„ `Linear` å±‚ï¼Œä½†æˆ‘ä»¬**å…³å¿ƒçš„æ˜¯ LoRA å¦‚ä½•åœ¨ `nn.Linear` å±‚ä¸Šé¢åšæ”¹åŠ¨**ï¼Œå› æ­¤åº”è¯¥çœ‹ `_create_new_module` æ–¹æ³•

```python
class LoraModel(BaseTuner):
    def _create_new_module(lora_config, adapter_name, target, **kwargs):
        if loaded_in_8bit and isinstance(target, bnb.nn.Linear8bitLt):
            ...
        else:
            if isinstance(target, torch.nn.Linear):
                # æ‹·è´æœ¬æ¥çš„æƒé‡çŸ©é˜µçš„ in_features å’Œ out_features å±æ€§
                in_features, out_features = target.in_features, target.out_features
            elif isinstance(target, Conv1D):
				...
            else:
				...
            new_module = Linear(
                adapter_name,
                in_features,
                out_features,
                bias=bias,
                **kwargs
            )

        return new_module

```

é€šè¿‡æ‹·è´æœ¬æ¥çš„ `nn.Linear` çš„ `in_features` å’Œ `out_features` å±æ€§ï¼ŒLoRA åˆ›å»ºäº†ä¸€ä¸ª `Linear` ç±»ï¼Œåœ¨åŒä¸ªæ–‡ä»¶ä¸­å¯ä»¥æ‰¾åˆ°è¿™ä¸ªç±»çš„å®šä¹‰

```python
class Linear(nn.Linear, LoraLayer):
    def __init__(
        self,
        adapter_name: str,
        in_features: int,
        out_features: int,
        r: int = 0,
        lora_alpha: int = 1,
        lora_dropout: float = 0.0,
        fan_in_fan_out: bool = False,
        is_target_conv_1d_layer: bool = False,
        **kwargs,
    ):
        init_lora_weights = kwargs.pop("init_lora_weights", True)

        nn.Linear.__init__(self, in_features, out_features, **kwargs)
        LoraLayer.__init__(self, in_features=in_features, out_features=out_features)
        # Freezing the pre-trained weight matrix
        self.weight.requires_grad = False

        nn.Linear.reset_parameters(self)
        self.update_layer(adapter_name, r, lora_alpha, lora_dropout, init_lora_weights)
        self.active_adapter = adapter_name

```

`Linear` ç±»é‡‡ç”¨äº† Mixin çš„è®¾è®¡æ¨¡å¼ï¼Œåˆ†åˆ«è°ƒç”¨äº† `nn.Linear` å’Œ `LoraLayer` çš„æ„é€ å‡½æ•°ï¼Œæ³¨æ„è¿™é‡Œçš„ `self.weight` æ˜¯æ¨¡å‹æœ¬æ¥çš„ `nn.Linear` å±‚çš„æƒé‡ï¼Œå¯ä»¥çœ‹åˆ°è¢«å†»ç»“äº†ï¼Œè€Œ LoRA å¼•å…¥çš„æƒé‡çŸ©é˜µåˆ™æ˜¯ç”¨ `update_layer` æ–¹æ³•è®¾ç½®ï¼Œæœç´¢è¯¥æ–¹æ³•å°†æˆ‘ä»¬å®šä½åˆ°äº† `LoraLayer` ç±»

```python
class LoraLayer(BaseTunerLayer):
    def __init__(self, in_features: int, out_features: int, **kwargs):
        self.r = {}
        self.lora_alpha = {}
        self.scaling = {}
        self.lora_dropout = nn.ModuleDict({})
        self.lora_A = nn.ModuleDict({})
        self.lora_B = nn.ModuleDict({})
        # For Embedding layer
        self.lora_embedding_A = nn.ParameterDict({})
        self.lora_embedding_B = nn.ParameterDict({})
        # Mark the weight as unmerged
        self.merged = False
        self.disable_adapters = False
        self.in_features = in_features
        self.out_features = out_features
        self.kwargs = kwargs

    def update_layer(
        self, adapter_name, r, lora_alpha, lora_dropout, init_lora_weights
    ):
        self.r[adapter_name] = r
        self.lora_alpha[adapter_name] = lora_alpha

        if lora_dropout > 0.0:
            lora_dropout_layer = nn.Dropout(p=lora_dropout)
        else:
            lora_dropout_layer = nn.Identity()

        self.lora_dropout.update(nn.ModuleDict({adapter_name: lora_dropout_layer}))
        # Actual trainable parameters
        if r > 0:
            self.lora_A.update(
                nn.ModuleDict(
                    {adapter_name: nn.Linear(self.in_features, r, bias=False)}
                )
            )
            self.lora_B.update(
                nn.ModuleDict(
                    {adapter_name: nn.Linear(r, self.out_features, bias=False)}
                )
            )
            self.scaling[adapter_name] = lora_alpha / r

        if init_lora_weights:
            self.reset_lora_parameters(adapter_name)

        self.to(self.weight.device)

```

ç»ˆäºï¼Œçœ‹åˆ°äº† LoRA è®¾ç½®é™ç»´çŸ©é˜µ A å’Œå‡ç»´çŸ©é˜µ B çš„åœ°æ–¹ï¼Œä¸Šé¢çš„ä»£ç è¿˜è®¾ç½®äº†æ”¾ç¼©å› å­ `lora_alpha / r`ï¼Œåªå‰©ä¸‹æœ€åä¸€ä¸ªé—®é¢˜ï¼Œå³ - **å‰å‘ä¼ æ’­çš„æ—¶å€™æ˜¯æ€ä¹ˆå·¥ä½œçš„**ï¼Ÿ

```python
class LoraLayer(BaseTunerLayer):
    def forward(self, x: torch.Tensor):
        previous_dtype = x.dtype
        if self.disable_adapters:
            ...
        elif self.r[self.active_adapter] > 0 and not self.merged:
            result = F.linear(
                x, transpose(self.weight, self.fan_in_fan_out), bias=self.bias
            )

            x = x.to(self.lora_A[self.active_adapter].weight.dtype)

            result += (
                self.lora_B[self.active_adapter](
                    self.lora_A[self.active_adapter](
                        self.lora_dropout[self.active_adapter](x)
                    )
                )
                * self.scaling[self.active_adapter]
            )
        else:
            ...

        result = result.to(previous_dtype)

        return result

```

çœ‹ä»£ç å°±å¾ˆæ¸…æ™°ï¼Œè¾“å…¥ `x` ä¼šä½œä¸ºæ¨¡å‹æœ¬æ¥çš„ `nn.Linear` ä»¥åŠ LoRA æ¨¡å—çš„è¾“å…¥ï¼ŒLoRA æ¨¡å—æœ€åè¿˜ä¼šè¿›è¡Œç¼©æ”¾ï¼Œä¸€åˆ‡éƒ½è·Ÿæœ¬æ–‡ä¸€å¼€å§‹è§£é‡Šçš„ä¸€æ ·